<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | Codex]]></title>
  <link href="http://wubr2000.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://wubr2000.github.io/"/>
  <updated>2014-06-23T00:14:34-07:00</updated>
  <id>http://wubr2000.github.io/</id>
  <author>
    <name><![CDATA[Bruno Wu]]></name>
    <email><![CDATA[wubr2000@hotmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Gradient Descent the Python Way]]></title>
    <link href="http://wubr2000.github.io/blog/2014/05/27/bayesian-invasion/"/>
    <updated>2014-05-27T22:53:07-07:00</updated>
    <id>http://wubr2000.github.io/blog/2014/05/27/bayesian-invasion</id>
    <content type="html"><![CDATA[<p>During our third week at Zipfian, we implemented the linear regression model by using various Python libraries (e.g. <code>statsmodel</code> and <code>scikit-learn</code>). We also coded the normal equation (<script type="math/tex">\beta = (X^TX)^{-1}X^TY</script>) directly as a Python function. In addition to using standard library functions to perform linear regressions, we also implemented an optimization technique called <strong><a href="http://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a></strong> to approximate the analytical solution for deriving the coefficients of a linear regression. When a matrix is non-invertible, one would have to use gradient descent to arrive at the coefficients of a linear regression since a <a href="http://stats.stackexchange.com/questions/69442/linear-regression-and-non-invertibility">unique solution does not exist</a> in this case. In fact, gradient descent is a general optimization technique for finding the local minimum of a function and as such, can be applied to many other machine learning situations where an analytical solution is either too cumbersome or is infeasible. So I thought it&rsquo;d be quite useful to get a better handle on this important tool.</p>

<p>Here&rsquo;s the general gradient descent algorithm:</p>

<script type="math/tex; mode=display">
\begin{align*}
  \theta_{j+1} = \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)
\end{align*}
</script>

<p>where <script type="math/tex">J(\theta)</script> is a function that we want to minimize and <script type="math/tex">\alpha</script> is the <em>learning rate</em>.</p>

<!-- more -->

<p>There is a <a href="https://class.coursera.org/ml-003/lecture/10">video</a> by Andrew Ng of Stanford on the basic intuition behind gradient descent. </p>

<blockquote>
  <p>The general idea of gradient descent is that we take the gradient (i.e. slope or first-derivation) of the function we want to minimize at some starting point. Then we take one step in the <strong>negative direction</strong> of the gradient (hence, a descent) and repeat this process many times. Eventually, the algorithm will converge to a point where the gradient is zero and where the function is, therefore, at a local minimum. The <script type="math/tex">\alpha</script> (or the learning rate) of the gradient descent algorithm determines how big a step we take at each iteration.</p>
</blockquote>

<p>For the case of linear regression, the function we want to minimize is the RSS (or the cost function):</p>

<script type="math/tex; mode=display">
\begin{align*}
  RSS = \displaystyle \sum_{i=1}^{n} (y_i - f(x_i))^2
\end{align*}
</script>

<p>By plugging the RSS (as <script type="math/tex">J(\theta)</script>) into the general gradient descent algorithm and doing some math (as explained on pages 4 and 5 of Andrew Ng&rsquo;s lecture notes <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">here</a>), we arrive at the following:</p>

<script type="math/tex; mode=display">
\begin{align*}
  \theta_{j+1} = \theta_j - \alpha \frac{1}{m} \displaystyle \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)}))x_j^{(i)}
\end{align*}
</script>

<p>where <script type="math/tex">h_{\theta}(x^{(i)})</script> is the predicted value (<script type="math/tex">\hat{y}</script>) of <script type="math/tex">x^{(i)}</script> and <script type="math/tex">y^{(i)}</script> is the true <script type="math/tex">y</script> value.</p>

<p>Once we have tailored the gradient descent algorithm for linear regression, the Python implementation of the coefficient <code>update</code> function is not too difficult. We just need to make sure that the matrix multiplications are done properly:</p>

<p><code>python
def update(self, X, y, alpha):
  gradient = X.T.dot(X.dot(self.coeffs)-y)
  m = len(y)
  return self.coeffs - alpha * gradient / m
</code></p>

<p>Next, we create a <code>run</code> function to iterate through the above gradient descent <code>update</code> function in order to arrive at a set of coefficient vector that would minimize the RSS for the linear regression. To do this, we need to pass in <script type="math/tex">\alpha</script> to control how <em>large</em> a step we&rsquo;d want to take from one iteration to the next:</p>

<p><code>python
def run(self, X, y, alpha=0.01, num_iterations=10000):
  for i in xrange(num_iterations):
    self.coeffs = self.update(X,y,alpha)
</code></p>

<p>Finally, we can use this set of coefficients vector for our linear regression (<script type="math/tex">\hat{y} = \beta X</script>):</p>

<p><code>python
def predict(self, X):
  return X.dot(self.coeffs)
</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Exploring NLP in Python]]></title>
    <link href="http://wubr2000.github.io/blog/2013/12/29/exploring-nlp-in-python/"/>
    <updated>2013-12-29T17:46:50-08:00</updated>
    <id>http://wubr2000.github.io/blog/2013/12/29/exploring-nlp-in-python</id>
    <content type="html"><![CDATA[<p>Came across a <a href="http://blog.scripted.com/scripted-updates/nlp-hacking-in-python/">great tutorial</a> on the basics of natural language processing (NLP) and classification. </p>

<p>The example in this tutorial uses a Python library called <code>gensim</code> which (according to its <a href="http://radimrehurek.com/gensim/about.html">website</a>) is the &ldquo;the most robust, efficient and hassle-free piece of software to realize unsupervised semantic modelling from plain text.&rdquo; As far as I understand it, it&rsquo;s a very handy and commonly-used library for NLP.</p>

<p>One important tool/algorithm used for classification is something called the <a href="http://en.wikipedia.org/wiki/Vector_space_model">Vector Space Model</a>. Basically, all documents or queries are represented as &ldquo;vectors of identifiers&rdquo;, such as an index of words and use the angle (theta) between vectors as a similarity measure (explained further later). Incidentally, if you ranked the similarity of each document to a query, you&rsquo;d have a search engine.</p>

<!-- more -->

<p>A common way to represent a written document as a vector is to think about it in terms of <a href="http://en.wikipedia.org/wiki/Bag-of-words_model">Bag of Words</a> (BoW) vectors. For example, if an entire document consists only of the words &ldquo;dog&rdquo; and &ldquo;cat&rdquo;, then the BoW vector for this document would be [# of &lsquo;dog&rsquo;, # of &lsquo;cat&rsquo;].</p>

<p>A very high-level overview of the workflow for NLP using the Vector Space Model is as follows:</p>

<blockquote>
  <p>Preprocessing -&gt; Create &ldquo;Bag of Words&rdquo; vector -&gt; Dimensionality Reduction -&gt; use SVM algorithm for classification</p>
</blockquote>

<p>A more detailed workflow is as follows:</p>

<blockquote>
  <p>Remove stopwords and split on spaces -&gt; Take out rare terms using <code>gensim</code> -&gt; Create Bag of Words vectors -&gt; Dimensionality reduction using LSI (<a href="http://en.wikipedia.org/wiki/Latent_semantic_indexing">Latent Semantic Indexing</a>) model in <code>gensim</code> (topic vectorization) -&gt; unit vectorization -&gt; finding cosine distance</p>
</blockquote>

<p>Several important concepts to be aware of from this tutorial:</p>

<ol>
  <li>
    <p><strong>Vector Space Model</strong></p>
  </li>
  <li><strong><a href="http://en.wikipedia.org/wiki/Curse_of_dimensionality">The Curse of Dimensionality</a></strong>
    <ul>
      <li>
        <blockquote>
          <p>&ldquo;There are all kinds of terrible things that happen as the dimensionality of your descriptor vectors rises. One obvious one is that as the dimensionality rises, both the time and space complexity of dealing with these vectors rises, often exponentially. Another issue is that as dimensionality rises, the amount of samples needed to draw useful conclusions from that data also rises steeply. Another way of phrasing that is with a fixed number of samples, the usefulness of each dimension diminishes. Finally, as the dimensionality rises, your points all tend to start becoming equidistant to each other, making it difficult to draw solid conclusions from them.&rdquo;</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li><strong>Similarity in Vector Space</strong>
    <ul>
      <li><em>Euclidean distance</em> (this ignores direction)</li>
      <li><em>Cosine distance</em> - measuring similarity based on angle between vectors is know as cosine distance, or cosine similarity. </li>
      <li><em>Unit vectorization</em> - modify the vectors themselves by dividing each number in each vector by that vector&rsquo;s magnitude. In doing so, all our vectors have a magnitude of 1. This process is called unit vectorization because the output vectors are units vectors.</li>
    </ul>
  </li>
  <li><strong>Supervised Learning</strong>
    <ul>
      <li>Train the algorithm on samples which have the &lsquo;correct&rsquo; answer provided with them. The specific supervised learning problem we&rsquo;re addressing here is called classification. You train an algorithm on labelled descriptor vectors, then ask it to label a previously unseen descriptor vector based on conclusions drawn from the training set.</li>
    </ul>
  </li>
  <li><strong><a href="http://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machine</a></strong> - SVM is a family of algorithms which define decision boundaries between classes based on labelled training data.
    <ul>
      <li>
        <blockquote>
          <p>&ldquo;For our &lsquo;dog&rsquo; vs. &lsquo;sandwich&rsquo; classification problem, we provide the algorithm with some training samples. These samples are documents which have gone through our whole process (BoW vector -&gt; topic vector -&gt; unit vector) and carry with them either a &lsquo;dog&rsquo; label or a &lsquo;sandwich&rsquo; label. As you provide the SVM model with these samples, it looks at these points in space and essentially draws a line between the &lsquo;sandwich&rsquo; documents and the &lsquo;dog&rsquo; documents. This border between &ldquo;dog&rdquo;-land and &ldquo;sandwich&rdquo;-land is known as a decision boundary. Whichever side of the line the query point falls on determines what the algorithm labels it.&rdquo;</p>
        </blockquote>
      </li>
      <li>
        <blockquote>
          <p>&ldquo;All samples in both training and test sets are labeled. However, in practice, you would build the model on the labeled training set, ignore the labels on the test set, feed them into the model, have the model guess what those labels are, and finally check whether or not the algorithm guessed correctly. This process of testing out your supervised learning algorithm with a training and test set is called cross-validation.&rdquo;</p>
        </blockquote>
      </li>
    </ul>
  </li>
</ol>

<p>```python
import warnings
warnings.filterwarnings(&lsquo;ignore&rsquo;, category=DeprecationWarning)
from math import sqrt
import gensim
from sklearn.svm import SVC
import os</p>

<p>def vec2dense(vec, num_terms):</p>

<pre><code>'''Convert from sparse gensim format to dense list of numbers'''
return list(gensim.matutils.corpus2dense([vec], num_terms=num_terms).T[0])
</code></pre>

<p>if <strong>name</strong> == &lsquo;<strong>main</strong>&rsquo;:</p>

<pre><code>#Load in corpus, remove newlines, make strings lower-case
docs = {}
corpus_dir = 'corpus'
for filename in os.listdir(corpus_dir):

    path = os.path.join(corpus_dir, filename)
    doc = open(path).read().strip().lower()
    docs[filename] = doc

names = docs.keys()

#Remove stopwords and split on spaces
print "\n---Corpus with Stopwords Removed---"

stop = ['the', 'of', 'a', 'at', 'is']
preprocessed_docs = {}
for name in names:

    text = docs[name].split()
    preprocessed = [word for word in text if word not in stop]
    preprocessed_docs[name] = preprocessed
    print name, ":", preprocessed

#Build the dictionary and filter out rare terms
dct = gensim.corpora.Dictionary(preprocessed_docs.values())
unfiltered = dct.token2id.keys()
dct.filter_extremes(no_below=2)
filtered = dct.token2id.keys()
filtered_out = set(unfiltered) - set(filtered)

print "\nThe following super common/rare words were filtered out..."
print list(filtered_out), '\n'

print "Vocabulary after filtering..."
print dct.token2id.keys(), '\n'

#Build Bag of Words Vectors out of preprocessed corpus
print "---Bag of Words Corpus---"

bow_docs = {}
for name in names:

    sparse = dct.doc2bow(preprocessed_docs[name])
    bow_docs[name] = sparse
    dense = vec2dense(sparse, num_terms=len(dct))
    print name, ":", dense

#Dimensionality reduction using LSI. Go from 6D to 2D.
print "\n---LSI Model---"

lsi_docs = {}
num_topics = 2
lsi_model = gensim.models.LsiModel(bow_docs.values(),
                                   num_topics=num_topics)
for name in names:

    vec = bow_docs[name]
    sparse = lsi_model[vec]
    dense = vec2dense(sparse, num_topics)
    lsi_docs[name] = sparse
    print name, ':', dense

#Normalize LSI vectors by setting each vector to unit length
print "\n---Unit Vectorization---"

unit_vecs = {}
for name in names:

    vec = vec2dense(lsi_docs[name], num_topics)
    norm = sqrt(sum(num ** 2 for num in vec))
    unit_vec = [num / norm for num in vec]
    unit_vecs[name] = unit_vec
    print name, ':', unit_vec

#Take cosine distances between docs and show best matches
print "\n---Document Similarities---"

index = gensim.similarities.MatrixSimilarity(lsi_docs.values())
for i, name in enumerate(names):

    vec = lsi_docs[name]
    sims = index[vec]
    sims = sorted(enumerate(sims), key=lambda item: -item[1])

    #Similarities are a list of tuples of the form (doc #, score)
    #In order to extract the doc # we take first value in the tuple
    #Doc # is stored in tuple as numpy format, must cast to int

    if int(sims[0][0]) != i:
        match = int(sims[0][0])
    else:
        match = int(sims[1][0])

    match = names[match]
    print name, "is most similar to...", match

#We add classes to the mix by labelling dog1.txt and sandwich1.txt
#We use these as our training set, and test on all documents.
print "\n---Classification---"

dog1 = unit_vecs['dog1.txt']
sandwich1 = unit_vecs['sandwich1.txt']

train = [dog1, sandwich1]

# The label '1' represents the 'dog' category
# The label '2' represents the 'sandwich' category

label_to_name = dict([(1, 'dogs'), (2, 'sandwiches')])
labels = [1, 2]
classifier = SVC()
classifier.fit(train, labels)

for name in names:

    vec = unit_vecs[name]
    label = classifier.predict([vec])[0]
    cls = label_to_name[label]
    print name, 'is a document about', cls

print '\n' ```
</code></pre>
]]></content>
  </entry>
  
</feed>
